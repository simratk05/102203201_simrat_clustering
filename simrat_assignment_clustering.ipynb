{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z3OhnWsCyHHP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data"
      ],
      "metadata": {
        "id": "OqqWqNX3yWXP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def no_processing(X): return X\n",
        "def normalization(X): return MinMaxScaler().fit_transform(X)\n",
        "def transform(X): return StandardScaler().fit_transform(X)\n",
        "def pca(X): return PCA(n_components=2).fit_transform(X)\n",
        "def t_n(X): return StandardScaler().fit_transform(MinMaxScaler().fit_transform(X))\n",
        "def t_n_pca(X): return PCA(n_components=2).fit_transform(StandardScaler().fit_transform(MinMaxScaler().fit_transform(X)))\n",
        "\n",
        "preprocessing_methods = {\n",
        "    \"No Data Processing\": no_processing,\n",
        "    \"Using Normalization\": normalization,\n",
        "    \"Using Transform\": transform,\n",
        "    \"Using PCA\": pca,\n",
        "    \"Using T+N\": t_n,\n",
        "    \"T+N+PCA\": t_n_pca\n",
        "}"
      ],
      "metadata": {
        "id": "VZC5CofyyWZ6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cluster_nums = [3, 4, 5]\n",
        "results = {\n",
        "    \"KMeans\": {},\n",
        "    \"Agglomerative\": {},\n",
        "    \"MeanShift\": {}\n",
        "}"
      ],
      "metadata": {
        "id": "8u0Ai2KayWiQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KMeans and Agglomerative\n",
        "for prep_name, prep_func in preprocessing_methods.items():\n",
        "    X_prep = prep_func(X)\n",
        "    for c in cluster_nums:\n",
        "        # KMeans\n",
        "        try:\n",
        "            km = KMeans(n_clusters=c, n_init=10, random_state=42)\n",
        "            labels_km = km.fit_predict(X_prep)\n",
        "            sil_km = silhouette_score(X_prep, labels_km)\n",
        "            cal_km = calinski_harabasz_score(X_prep, labels_km)\n",
        "            dav_km = davies_bouldin_score(X_prep, labels_km)\n",
        "        except Exception:\n",
        "            sil_km = cal_km = dav_km = \"NA\"\n",
        "        results[\"KMeans\"].setdefault(prep_name, []).append((sil_km, cal_km, dav_km))\n",
        "\n",
        "        # Agglomerative\n",
        "        try:\n",
        "            agg = AgglomerativeClustering(n_clusters=c)\n",
        "            labels_agg = agg.fit_predict(X_prep)\n",
        "            sil_agg = silhouette_score(X_prep, labels_agg)\n",
        "            cal_agg = calinski_harabasz_score(X_prep, labels_agg)\n",
        "            dav_agg = davies_bouldin_score(X_prep, labels_agg)\n",
        "        except Exception:\n",
        "            sil_agg = cal_agg = dav_agg = \"NA\"\n",
        "        results[\"Agglomerative\"].setdefault(prep_name, []).append((sil_agg, cal_agg, dav_agg))\n",
        "\n",
        "        # MeanShift (only for c=3 to match your table, since MeanShift doesn't use n_clusters)\n",
        "        if c == 3:\n",
        "            try:\n",
        "                ms = MeanShift()\n",
        "                labels_ms = ms.fit_predict(X_prep)\n",
        "                n_clusters_ms = len(np.unique(labels_ms))\n",
        "                if n_clusters_ms == 3:  # Only record if 3 clusters found\n",
        "                    sil_ms = silhouette_score(X_prep, labels_ms)\n",
        "                    cal_ms = calinski_harabasz_score(X_prep, labels_ms)\n",
        "                    dav_ms = davies_bouldin_score(X_prep, labels_ms)\n",
        "                else:\n",
        "                    sil_ms = cal_ms = dav_ms = \"NA\"\n",
        "            except Exception:\n",
        "                sil_ms = cal_ms = dav_ms = \"NA\"\n",
        "            results[\"MeanShift\"].setdefault(prep_name, []).append((sil_ms, cal_ms, dav_ms))\n",
        "        else:\n",
        "            results[\"MeanShift\"].setdefault(prep_name, []).append((\"NA\", \"NA\", \"NA\"))"
      ],
      "metadata": {
        "id": "mhQTJe0PyWk-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_results_dataframe(algorithm, preprocessing_methods, cluster_nums, results):\n",
        "    rows = []\n",
        "    for metric_idx, metric_name in enumerate([\"Silhouette\", \"Calinski-Harabasz\", \"Davies-Bouldin\"]):\n",
        "        row = {\"Parameters\": metric_name}\n",
        "        for prep in preprocessing_methods:\n",
        "            for cidx, c in enumerate(cluster_nums):\n",
        "                value = results[algorithm][prep][cidx][metric_idx]\n",
        "                row[f\"{prep}\\nc={c}\"] = np.round(value, 3) if value != \"NA\" else np.nan\n",
        "        rows.append(row)\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "df_kmeans = create_results_dataframe(\"KMeans\", list(preprocessing_methods.keys()), cluster_nums, results)\n",
        "df_agg = create_results_dataframe(\"Agglomerative\", list(preprocessing_methods.keys()), cluster_nums, results)\n",
        "df_meanshift = create_results_dataframe(\"MeanShift\", list(preprocessing_methods.keys()), cluster_nums, results)\n",
        "\n"
      ],
      "metadata": {
        "id": "WpJrKpF_yW6c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUNY3bvv2swr",
        "outputId": "082bb10c-f75d-4035-8285-992384258758"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Parameters  No Data Processing\\nc=3  No Data Processing\\nc=4  \\\n",
            "0         Silhouette                    0.553                    0.498   \n",
            "1  Calinski-Harabasz                  561.628                  530.766   \n",
            "2     Davies-Bouldin                    0.662                    0.780   \n",
            "\n",
            "   No Data Processing\\nc=5  Using Normalization\\nc=3  \\\n",
            "0                    0.491                     0.505   \n",
            "1                  495.370                   359.845   \n",
            "2                    0.816                     0.760   \n",
            "\n",
            "   Using Normalization\\nc=4  Using Normalization\\nc=5  Using Transform\\nc=3  \\\n",
            "0                     0.445                     0.353                 0.460   \n",
            "1                   314.473                   289.506               241.904   \n",
            "2                     0.900                     0.957                 0.834   \n",
            "\n",
            "   Using Transform\\nc=4  Using Transform\\nc=5  Using PCA\\nc=3  Using PCA\\nc=4  \\\n",
            "0                 0.387                 0.346           0.598           0.558   \n",
            "1               207.266               202.952         693.708         719.124   \n",
            "2                 0.870                 0.948           0.565           0.615   \n",
            "\n",
            "   Using PCA\\nc=5  Using T+N\\nc=3  Using T+N\\nc=4  Using T+N\\nc=5  \\\n",
            "0           0.552           0.460           0.387           0.346   \n",
            "1         685.027         241.904         207.266         202.952   \n",
            "2           0.632           0.834           0.870           0.948   \n",
            "\n",
            "   T+N+PCA\\nc=3  T+N+PCA\\nc=4  T+N+PCA\\nc=5  \n",
            "0         0.509         0.441         0.416  \n",
            "1       293.857       264.488       278.549  \n",
            "2         0.710         0.755         0.771  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_agg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2muuHF572stM",
        "outputId": "d6ca9864-e1e3-4dff-8875-b9186b8cc97d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Parameters  No Data Processing\\nc=3  No Data Processing\\nc=4  \\\n",
            "0         Silhouette                    0.554                    0.489   \n",
            "1  Calinski-Harabasz                  558.058                  515.079   \n",
            "2     Davies-Bouldin                    0.656                    0.795   \n",
            "\n",
            "   No Data Processing\\nc=5  Using Normalization\\nc=3  \\\n",
            "0                    0.484                     0.505   \n",
            "1                  488.485                   349.254   \n",
            "2                    0.820                     0.748   \n",
            "\n",
            "   Using Normalization\\nc=4  Using Normalization\\nc=5  Using Transform\\nc=3  \\\n",
            "0                     0.433                     0.349                 0.447   \n",
            "1                   301.104                   272.024               222.719   \n",
            "2                     0.849                     0.906                 0.803   \n",
            "\n",
            "   Using Transform\\nc=4  Using Transform\\nc=5  Using PCA\\nc=3  Using PCA\\nc=4  \\\n",
            "0                 0.401                 0.331           0.598           0.541   \n",
            "1               201.251               192.681         688.618         673.946   \n",
            "2                 0.979                 0.974           0.560           0.655   \n",
            "\n",
            "   Using PCA\\nc=5  Using T+N\\nc=3  Using T+N\\nc=4  Using T+N\\nc=5  \\\n",
            "0           0.549           0.447           0.401           0.331   \n",
            "1         665.883         222.719         201.251         192.681   \n",
            "2           0.653           0.803           0.979           0.974   \n",
            "\n",
            "   T+N+PCA\\nc=3  T+N+PCA\\nc=4  T+N+PCA\\nc=5  \n",
            "0         0.511         0.449         0.404  \n",
            "1       286.329       254.090       254.996  \n",
            "2         0.705         0.723         0.791  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_meanshift)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlIogmZayW8z",
        "outputId": "044e6f93-865c-4476-8fcd-929d99681976"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Parameters  No Data Processing\\nc=3  No Data Processing\\nc=4  \\\n",
            "0         Silhouette                      NaN                      NaN   \n",
            "1  Calinski-Harabasz                      NaN                      NaN   \n",
            "2     Davies-Bouldin                      NaN                      NaN   \n",
            "\n",
            "   No Data Processing\\nc=5  Using Normalization\\nc=3  \\\n",
            "0                      NaN                       NaN   \n",
            "1                      NaN                       NaN   \n",
            "2                      NaN                       NaN   \n",
            "\n",
            "   Using Normalization\\nc=4  Using Normalization\\nc=5  Using Transform\\nc=3  \\\n",
            "0                       NaN                       NaN                   NaN   \n",
            "1                       NaN                       NaN                   NaN   \n",
            "2                       NaN                       NaN                   NaN   \n",
            "\n",
            "   Using Transform\\nc=4  Using Transform\\nc=5  Using PCA\\nc=3  Using PCA\\nc=4  \\\n",
            "0                   NaN                   NaN             NaN             NaN   \n",
            "1                   NaN                   NaN             NaN             NaN   \n",
            "2                   NaN                   NaN             NaN             NaN   \n",
            "\n",
            "   Using PCA\\nc=5  Using T+N\\nc=3  Using T+N\\nc=4  Using T+N\\nc=5  \\\n",
            "0             NaN             NaN             NaN             NaN   \n",
            "1             NaN             NaN             NaN             NaN   \n",
            "2             NaN             NaN             NaN             NaN   \n",
            "\n",
            "   T+N+PCA\\nc=3  T+N+PCA\\nc=4  T+N+PCA\\nc=5  \n",
            "0           NaN           NaN           NaN  \n",
            "1           NaN           NaN           NaN  \n",
            "2           NaN           NaN           NaN  \n"
          ]
        }
      ]
    }
  ]
}